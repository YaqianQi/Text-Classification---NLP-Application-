{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main approach NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rule based methods\n",
    "    - Regular expression \n",
    "    - context-free grammars \n",
    "2. Probabilistic mdoeling and machine learning \n",
    "   - likelihood maximiaztion \n",
    "   - linear classifier: \n",
    "     - logistic regression : y_hat = 1/ (1+ e^-(w*x + b)) , w:coef? \n",
    "3. deep Learning \n",
    "   - stack of neuron: activation_function(w*x) , logit, relu \n",
    " .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "s = \"I love apples, don't you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'apples,', \"don't\", 'you?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'apples', ',', 'do', \"n't\", 'you', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'apples', ',', 'don', \"'\", 't', 'you', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: remove and replacing suffixes to get the root form of the word \n",
    "\n",
    "example \n",
    "\n",
    "cats -> cat\n",
    "\n",
    "cares -> care\n",
    "\n",
    "talked -> talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemming \n",
    "text = \"feet cats wolves talked\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet cat wolv talk'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization \n",
    "**Function**: return the base or dictionary form of words \n",
    "\n",
    "Example: \n",
    "\n",
    "feet -> foot\n",
    "\n",
    "wolve -> wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aliciaqi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'foot cat wolf talked'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words \n",
    "\n",
    "\n",
    "<img src=\"pics/bag of words.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \" I love apples, don't you. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_gram = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love'], ['love', 'apples,'], ['apples,', \"don't\"], [\"don't\", 'you.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "lst = s.split()\n",
    "for i in range(len(lst)-1):\n",
    "    res.append([lst[i],lst[i+1]])\n",
    "res   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/n-gram.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifiers for character in Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My phone number is 408-555-1234. I am avaible from 11:00am to 4:00pm on business day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'phone' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 8), match='phone'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'phone'\n",
    "re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 8), match='phone'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = re.search(pattern,text)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Primary phone:343-234-2343 , second phone: 123-343-2343 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(8, 13), match='phone'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = re.search(\"phone\",text2)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phone', 'phone']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = re.findall(\"phone\",text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 13)\n",
      "(36, 41)\n"
     ]
    }
   ],
   "source": [
    "for match in re.finditer(\"phone\",text2):\n",
    "    print(match.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >\\d</span></td><td>A digit</td><td>file_\\d\\d</td><td>file_25</td></tr>\n",
    "\n",
    "<tr ><td><span >\\w</span></td><td>Alphanumeric</td><td>\\w-\\w\\w\\w</td><td>A-b_1</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\s</span></td><td>White space</td><td>a\\sb\\sc</td><td>a b c</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\D</span></td><td>A non digit</td><td>\\D\\D\\D</td><td>ABC</td></tr>\n",
    "\n",
    "<tr ><td><span >\\W</span></td><td>Non-alphanumeric</td><td>\\W\\W\\W\\W\\W</td><td>*-+=)</td></tr>\n",
    "\n",
    "<tr ><td><span >\\S</span></td><td>Non-whitespace</td><td>\\S\\S\\S\\S</td><td>Yoyo</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = re.search(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (232)-(323)-(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408-555-1234'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone.group() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >+</span></td><td>Occurs one or more times</td><td>\tVersion \\w-\\w+</td><td>Version A-b1_1</td></tr>\n",
    "\n",
    "<tr ><td><span >{3}</span></td><td>Occurs exactly 3 times</td><td>\\D{3}</td><td>abc</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{2,4}</span></td><td>Occurs 2 to 4 times</td><td>\\d{2,4}</td><td>123</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{3,}</span></td><td>Occurs 3 or more</td><td>\\w{3,}</td><td>anycharacters</td></tr>\n",
    "\n",
    "<tr ><td><span >\\*</span></td><td>Occurs zero or more times</td><td>A\\*B\\*C*</td><td>AAACC</td></tr>\n",
    "\n",
    "<tr ><td><span >?</span></td><td>Once or none</td><td>plurals?</td><td>plural</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408-555-1234'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = re.search(r'\\d{3}-\\d{3}-\\d{4}',text)\n",
    "matches.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dafdajk@gmail.com']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# email \n",
    "pattern = r'\\w+@\\w+.\\w{3}'\n",
    "email_string = \"dafdajk@gmail.com\"\n",
    "re.findall(pattern, email_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "*https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting* \n",
    "\n",
    "\n",
    "*https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76*\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"pics/tfidf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text1 = \"I love Apple\"\n",
    "\n",
    "text2 = \"I love sunshine\"\n",
    "\n",
    "text3 = \"I hate chocolate\" \n",
    "\n",
    "love in text 1: tf = 1 , idf = log(3/2) , tf-idf = 0.4 * 1 = 0.4 \n",
    "\n",
    "sunshine in text2: tf = 1, idf = log(3/1), tf-idf = 1 * 1.09 = 1.09\n",
    "\n",
    "=> sunshine less frequency but higher tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    # change white space to one \n",
    "    text=re.sub(\"\\s+\",\" \",text)\n",
    "    # remove start and end white spaces \n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love apple', 'i love sunshine', 'i hate chocolate']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I love Apple\"\n",
    "text2 = \"I love sunshine\"\n",
    "text3 = \"I hate chocolate\" \n",
    "corpus = []\n",
    "corpus.append(text1)\n",
    "corpus.append(text2)\n",
    "corpus.append(text3)\n",
    "text_sents_clean = [pre_process(s) for s in corpus]\n",
    "text_sents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love Apple', 'I love sunshine', 'I hate chocolate']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# corpus\n",
    "vectorizer =TfidfVectorizer(ngram_range=(1,2)) #N_GRAM \n",
    "features = vectorizer.fit_transform(corpus)\n",
    "tf_idf_df = pd.DataFrame(features.todense(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.62276601, 0.        , 0.        , 0.        , 0.4736296 ,\n",
       "         0.62276601, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.4736296 ,\n",
       "         0.        , 0.62276601, 0.62276601],\n",
       "        [0.        , 0.57735027, 0.57735027, 0.57735027, 0.        ,\n",
       "         0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_tfidf_sklearn(corpus, N_GRAM=1):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # corpus\n",
    "    vectorizer =TfidfVectorizer(ngram_range=(N_GRAM,N_GRAM)) #N_GRAM \n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    res = pd.DataFrame(features.todense(), columns=vectorizer.get_feature_names())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_tfidf_sklearn(corpus, N_GRAM=(1,1)):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # corpus\n",
    "    vectorizer =TfidfVectorizer(ngram_range=N_GRAM) #N_GRAM \n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    res = pd.DataFrame(features.todense(), columns=vectorizer.get_feature_names())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>hate</th>\n",
       "      <th>hate chocolate</th>\n",
       "      <th>love</th>\n",
       "      <th>love apple</th>\n",
       "      <th>love sunshine</th>\n",
       "      <th>sunshine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.47363</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.47363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.622766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      apple  chocolate     hate  hate chocolate     love  love apple  \\\n",
       "0  0.622766    0.00000  0.00000         0.00000  0.47363    0.622766   \n",
       "1  0.000000    0.00000  0.00000         0.00000  0.47363    0.000000   \n",
       "2  0.000000    0.57735  0.57735         0.57735  0.00000    0.000000   \n",
       "\n",
       "   love sunshine  sunshine  \n",
       "0       0.000000  0.000000  \n",
       "1       0.622766  0.622766  \n",
       "2       0.000000  0.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "662.727px",
    "left": "25px",
    "top": "302.756px",
    "width": "204.625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
